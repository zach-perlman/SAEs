# Default configuration for SAE experiments

model:
  type: MatryoshkaSAE # VanillaSAE, GatedSAE, TopKSAE, JumpReLUSAE, RouteSAE, MatryoshkaSAE, RouteMatryoshkaSAE
  # For RouteSAE
  base_sae_type: MatryoshkaSAE
  start_layer: 9
  end_layer: 27
  # General model params
  hidden_size: 1024
  latent_size: 63488
  language_model_path: "Qwen/Qwen3-0.6B"
  n_layers: 28
  layer: 21 # For single-layer SAEs
  # For TopKSAE
  k: 50
  # For JumpReLUSAE
  threshold: 0.001
  bandwidth: 0.001
  # For MatryoshkaSAE
  group_sizes: [2048, 4096, 8192, 16384, 32768] # Must sum to latent_size (63488)
  group_weights: null # Optional: weights for each group in loss computation. If null, uses uniform weights
  # For RouteSAE
  aggre: 'sum' # 'sum' or 'mean'
  routing: 'hard' # 'hard' or 'soft'

training:
  batch_size: 64
  max_length: 512
  # Learning rate: Reference uses auto-scaling: lr = 2e-4 / sqrt(dict_size / 2^14)
  # For dict_size=63488: lr â‰ˆ 0.000127. Using slightly higher for stability.
  lr: 0.00015
  betas: [0.9, 0.999]
  num_epochs: 1
  steps: 500_000 # Number of training steps is required for streaming datasets
  seed: 42
  device: "cuda:0"
  l1_coefficient: 0.0005 # L1 sparsity penalty coefficient
  use_chat_template: True
  
  # Gradient accumulation for larger effective batch size
  gradient_accumulation_steps: 8 # Effective batch size = batch_size * gradient_accumulation_steps
  
  # Stability improvements from reference implementation
  gradient_clip_norm: 1.0 # Reduced from 5.0 to match reference (more conservative)
  unit_norm_frequency: 1 # Enforce every step (not every 10) - critical for Matryoshka
  num_warmup_steps: 500
  
  # Learned threshold for Matryoshka SAEs
  threshold_beta: 0.999 # EMA coefficient for threshold update
  threshold_start_step: 500 # When to start updating threshold
  
  # Early stopping
  early_stopping_patience: 100
  early_stopping_min_delta: 0.0001 # Minimum change in validation loss to be considered an improvement
  log_frequency: 15 # How often to print training stats

  # Dead feature revival
  dead_feature_threshold: 10_000_000 # Increased threshold
  auxk_alpha: 0.03125 # 1/32 as in reference implementation

data:
  train_path: "openbmb/ultra-fineweb"
  dataset_type: "ultrafineweb"
  dataset_split: "en"
  streaming: True
  max_samples: 500_000_000
  num_eval_samples: 1_000

wandb:
  use_wandb: False
  project: "sae-experiments"
  entity: ""
